{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net(\n",
      "  (input_bn): BatchNorm1d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (first_hidden): Linear(in_features=418, out_features=200, bias=True)\n",
      "  (first_bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (second_hidden): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (second_bn): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
      "), Net(\n",
      "  (input_bn): BatchNorm1d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (first_hidden): Linear(in_features=418, out_features=200, bias=True)\n",
      "  (first_bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (second_hidden): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (second_bn): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=100, out_features=2, bias=True)\n",
      ")]\n",
      "[Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-06\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-06\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "\n",
    "from MyNet import Net, MetaDataSet\n",
    "\n",
    "# hyper parameters\n",
    "BATCH_SIZE = 10\n",
    "EPOCH_SIZE = 10\n",
    "LR = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-6\n",
    "num_WORKERS = 1\n",
    "\n",
    "save_path = './'\n",
    "metadata_dir = 'E:/metadata数据集/new_bigmetadata/australian/query_time/'\n",
    "\n",
    "# net = Net(n_feature=396, first_n_hidden=100, second_n_hidden=20, n_output=2)\n",
    "net_bn = Net(n_feature=418, first_n_hidden=200, second_n_hidden=100, n_output=2, batch_normalization=True)\n",
    "net_no_bn = Net(n_feature=418, first_n_hidden=200, second_n_hidden=100, n_output=2, batch_normalization=False)\n",
    "\n",
    "nets = [net_bn, net_no_bn]\n",
    "optimizers = [optim.Adam(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY) for net in nets]\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(nets)\n",
    "print(optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the dataset....\n",
      "torch.float32\n",
      "torch.int64\n",
      "Dataset is ready!\n"
     ]
    }
   ],
   "source": [
    "print('Preparing the dataset....')\n",
    "loader = MetaDataSet(metadata_dir, BATCH_SIZE, num_WORKERS)\n",
    "print(\"Dataset is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net 0  [1,  1000] loss: 0.287\n",
      "net 1  [1,  1000] loss: 0.297\n",
      "net 0  [1,  2000] loss: 0.291\n",
      "net 1  [1,  2000] loss: 0.302\n",
      "net 0  [1,  3000] loss: 0.287\n",
      "net 1  [1,  3000] loss: 0.293\n",
      "net 0  [1,  4000] loss: 0.286\n",
      "net 1  [1,  4000] loss: 0.297\n",
      "net 0  [1,  5000] loss: 0.287\n",
      "net 1  [1,  5000] loss: 0.297\n",
      "net 0  [1,  6000] loss: 0.291\n",
      "net 1  [1,  6000] loss: 0.299\n",
      "net 0  [1,  7000] loss: 0.289\n",
      "net 1  [1,  7000] loss: 0.298\n",
      "net 0  [1,  8000] loss: 0.288\n",
      "net 1  [1,  8000] loss: 0.299\n",
      "net 0  [1,  9000] loss: 0.284\n",
      "net 1  [1,  9000] loss: 0.298\n",
      "net 0  [1, 10000] loss: 0.285\n",
      "net 1  [1, 10000] loss: 0.296\n",
      "net 0  [1, 11000] loss: 0.286\n",
      "net 1  [1, 11000] loss: 0.298\n",
      "net 0  [1, 12000] loss: 0.292\n",
      "net 1  [1, 12000] loss: 0.300\n",
      "net 0  [1, 13000] loss: 0.286\n",
      "net 1  [1, 13000] loss: 0.294\n",
      "net 0  [1, 14000] loss: 0.288\n",
      "net 1  [1, 14000] loss: 0.295\n",
      "net 0  [1, 15000] loss: 0.287\n",
      "net 1  [1, 15000] loss: 0.296\n",
      "net 0  [1, 16000] loss: 0.285\n",
      "net 1  [1, 16000] loss: 0.298\n",
      "net 0  [1, 17000] loss: 0.286\n",
      "net 1  [1, 17000] loss: 0.297\n",
      "net 0  [1, 18000] loss: 0.287\n",
      "net 1  [1, 18000] loss: 0.299\n",
      "net 0  [1, 19000] loss: 0.285\n",
      "net 1  [1, 19000] loss: 0.294\n",
      "net 0  [1, 20000] loss: 0.286\n",
      "net 1  [1, 20000] loss: 0.294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6fc16d99c1d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tarining the NN\n",
    "for epoch in range(EPOCH_SIZE):\n",
    "    running_loss0 = 0.0\n",
    "    running_loss1 = 0.0\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        for net, optimizer in zip(nets, optimizers):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if net == nets[0]:\n",
    "                i = 0\n",
    "                # print statistics\n",
    "                running_loss0 += loss.item()\n",
    "                # print every 2000 mini-batches\n",
    "                if step % 1000 == 999:    \n",
    "                    print('net',i,' [%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, step + 1, running_loss0 / 2000))\n",
    "                    running_loss0 = 0.0\n",
    "            else:\n",
    "                i = 1\n",
    "                # print statistics\n",
    "                running_loss1 += loss.item()\n",
    "                # print every 2000 mini-batches\n",
    "                if step % 1000 == 999:    \n",
    "                    print('net',i,' [%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, step + 1, running_loss1 / 2000))\n",
    "                    running_loss1 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "metadata = np.load('E:/metadata数据集/new_bigmetadata/australian/query_time/40australian30_big_metadata30.npy')\n",
    "net2 = nets[1]\n",
    "# X = metadata[:, 0:396]\n",
    "# y = metadata[:, 396]\n",
    "X = metadata[:, 0:418]\n",
    "y = metadata[:, 418]\n",
    "\n",
    "y[np.where(y>0)[0]] = 1\n",
    "y[np.where(y<=0)[0]] = 0\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "X_tensor = X_tensor.float()\n",
    "y_tensor = y_tensor.long()\n",
    "\n",
    "out = net2(X_tensor)\n",
    "print(F.softmax(out))\n",
    "pd = torch.max(F.softmax(out), 1)[1]\n",
    "print(pd)\n",
    "print(np.shape(pd))\n",
    "print('preidtion',sum(pd))\n",
    "print('groud true',sum(y_tensor))\n",
    "\n",
    "# print(pred)\n",
    "# pred_np=pred.detach().numpy()\n",
    "# print(pred_np)\n",
    "# predict = np.zeros_like(pred_np[:,0])\n",
    "# predict = pred_np[:,0] - pred_np[:,1]\n",
    "# predict[np.where(predict>0)[0]] = 1\n",
    "# predict[np.where(predict<=0)[0]] = 0\n",
    "\n",
    "# print(predict)\n",
    "# print(accuracy_score(y, predict))\n",
    "print(accuracy_score(y[index], pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the parameters in NN\n",
    "torch.save({\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, './net_parameters.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pyroch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
